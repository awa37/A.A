{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb7eafe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('alllines.txt', 'r') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d84c46df",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = text.lower().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48bd40a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
    "words = text_to_word_sequence(text)\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(words)\n",
    "vocabulary_size = len(tokenizer.word_index) + 1\n",
    "sequences = tokenizer.texts_to_sequences(lines)\n",
    "\n",
    "subsequences = []\n",
    "for sequence in sequences:\n",
    "    for i in range(1, len(sequence)):\n",
    "       subsequence = sequence[:i+1]\n",
    "       subsequences.append(subsequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "788091d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "sequence_length = max([len(sequence) for sequence in sequences])\n",
    "sequences = pad_sequences(subsequences, maxlen=sequence_length, padding='pre')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed7843aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 67.6 GiB for an array with shape (709820, 25576) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_categorical\n\u001b[0;32m      2\u001b[0m x, y \u001b[38;5;241m=\u001b[39m sequences[:,:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],sequences[:,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m----> 3\u001b[0m y \u001b[38;5;241m=\u001b[39m to_categorical(y, num_classes\u001b[38;5;241m=\u001b[39mvocabulary_size)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\np_utils.py:73\u001b[0m, in \u001b[0;36mto_categorical\u001b[1;34m(y, num_classes, dtype)\u001b[0m\n\u001b[0;32m     71\u001b[0m     num_classes \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(y) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     72\u001b[0m n \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 73\u001b[0m categorical \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((n, num_classes), dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m     74\u001b[0m categorical[np\u001b[38;5;241m.\u001b[39marange(n), y] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     75\u001b[0m output_shape \u001b[38;5;241m=\u001b[39m input_shape \u001b[38;5;241m+\u001b[39m (num_classes,)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 67.6 GiB for an array with shape (709820, 25576) and data type float32"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "x, y = sequences[:,:-1],sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3f1dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "model = Sequential()\n",
    "from keras.layers import Embedding\n",
    "model.add(Embedding(vocabulary_size, 100, input_length=sequence_length - 1))\n",
    "from keras.layers import LSTM\n",
    "model.add(LSTM(100))\n",
    "from keras.layers import Dropout\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(units=vocabulary_size, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7d0a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', epochs=500,  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd3bc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
    "words= text_to_word_sequence(text)\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(words)\n",
    "tokens = tokenizer.word_index\n",
    "vocabulary_size = len(tokenizer.word_index) + 1\n",
    "print(vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59591674",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361b1f88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
